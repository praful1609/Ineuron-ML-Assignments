{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9de80927",
   "metadata": {},
   "source": [
    "1. In a linear equation, the dependent variable and the independent variable have distinct roles:\n",
    "\n",
    "   - Dependent Variable: This is the variable that you want to predict or explain. It is often denoted as \"Y\" and is influenced by changes in the independent variable(s).\n",
    "\n",
    "   - Independent Variable: This is the variable that is used to explain or predict changes in the dependent variable. It is often denoted as \"X\" and is under your control or is considered the cause of changes in the dependent variable.\n",
    "\n",
    "2. Simple Linear Regression:\n",
    "   Simple linear regression is a statistical method used to model the relationship between a single independent variable and a dependent variable by fitting a linear equation to the observed data. The equation takes the form: Y = a + bX, where \"Y\" is the dependent variable, \"X\" is the independent variable, \"a\" is the intercept, and \"b\" is the slope.\n",
    "\n",
    "   Example: Suppose you want to predict a person's weight (Y) based on their height (X). You collect data on the heights and weights of several individuals and use simple linear regression to find the equation that best describes the relationship between height and weight.\n",
    "\n",
    "3. Slope in Linear Regression:\n",
    "   In linear regression, the slope (often denoted as \"b\") represents the change in the dependent variable (Y) for a one-unit change in the independent variable (X). It quantifies the strength and direction of the relationship between the two variables. A positive slope indicates a positive relationship (as X increases, Y also increases), while a negative slope indicates a negative relationship (as X increases, Y decreases).\n",
    "\n",
    "4. To determine the slope from the given points (3, 2) and (2, 2), you can use the formula for calculating the slope:\n",
    "\n",
    "   Slope (b) = (Y2 - Y1) / (X2 - X1)\n",
    "   Slope (b) = (2 - 2) / (2 - 3)\n",
    "   Slope (b) = 0 / (-1)\n",
    "   Slope (b) = 0\n",
    "\n",
    "   The slope of the line passing through these two points is 0.\n",
    "\n",
    "5. Conditions for a Positive Slope in Linear Regression:\n",
    "   - As the independent variable (X) increases, the dependent variable (Y) also increases.\n",
    "   - The correlation between X and Y is positive (correlation coefficient > 0).\n",
    "\n",
    "6. Conditions for a Negative Slope in Linear Regression:\n",
    "   - As the independent variable (X) increases, the dependent variable (Y) decreases.\n",
    "   - The correlation between X and Y is negative (correlation coefficient < 0).\n",
    "\n",
    "7. Multiple Linear Regression:\n",
    "   Multiple linear regression is an extension of simple linear regression that involves modeling the relationship between a dependent variable and two or more independent variables. It aims to find the best-fit linear equation with multiple predictors. The equation takes the form: Y = a + b1X1 + b2X2 + ... + bnXn, where \"Y\" is the dependent variable, \"X1,\" \"X2,\" etc., are the independent variables, \"a\" is the intercept, and \"b1,\" \"b2,\" etc., are the slopes for each independent variable.\n",
    "\n",
    "8. Number of Squares Due to Error in Multiple Linear Regression:\n",
    "   The number of squares due to error (SSE) in multiple linear regression measures the sum of the squared differences between the observed values and the predicted values of the dependent variable. It quantifies the unexplained variability in the data.\n",
    "\n",
    "9. Number of Squares Due to Regression in Multiple Linear Regression:\n",
    "   The number of squares due to regression (SSR) in multiple linear regression measures the sum of the squared differences between the predicted values and the mean of the dependent variable. It quantifies the explained variability in the data.\n",
    "\n",
    "10. Multicollinearity in Regression:\n",
    "    Multicollinearity is a condition in multiple linear regression where two or more independent variables are highly correlated with each other. It can cause problems in regression analysis, as it makes it difficult to isolate the individual effects of each independent variable on the dependent variable.\n",
    "\n",
    "11. Heteroskedasticity:\n",
    "    Heteroskedasticity is a term used in regression analysis to describe a situation where the variability of the errors (residuals) is not constant across all levels of the independent variable(s). In other words, the spread or dispersion of the residuals changes as the values of the independent variables change.\n",
    "\n",
    "12. Ridge Regression:\n",
    "    Ridge regression is a regularization technique used in linear regression to prevent overfitting by adding a penalty term to the regression equation. It adds a constraint that the sum of the squared coefficients should be less than a specified value, which helps to reduce the influence of individual predictors on the model.\n",
    "\n",
    "13. Lasso Regression:\n",
    "    Lasso regression, short for \"Least Absolute Shrinkage and Selection Operator,\" is another regularization technique in linear regression. It adds a penalty term that encourages some of the coefficient values to become exactly zero, effectively performing feature selection by eliminating less important predictors from the model.\n",
    "\n",
    "14. Polynomial Regression:\n",
    "    Polynomial regression is a form of regression analysis where the relationship between the dependent variable and one or more independent variables is modeled as an nth-degree polynomial. It allows for more complex and curved relationships to be captured in the regression equation.\n",
    "\n",
    "15. Basis Function:\n",
    "    A basis function is a mathematical function used in polynomial regression and other nonlinear regression techniques to transform the original independent variable(s) into a new set of variables. These new variables are used in the regression equation, allowing it to fit nonlinear relationships between variables.\n",
    "\n",
    "16. Logistic Regression:\n",
    "    Logistic regression is a statistical method used for binary classification problems. It models the probability of an event occurring (such as a yes/no outcome) based on one or more independent variables. Unlike linear regression, it uses the logistic function to constrain the predicted values between 0 and 1, making it suitable for classification tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
