{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26c5d8e8",
   "metadata": {},
   "source": [
    "1. **Difference Between Supervised and Unsupervised Learning:**\n",
    "\n",
    "   - **Supervised Learning:** In supervised learning, the algorithm is trained on a labeled dataset, where the input data is paired with corresponding target labels or outcomes. The goal is to learn a mapping from input to output, making predictions or classifications based on this learned relationship. Examples include:\n",
    "     - **Classification:** Predicting whether an email is spam or not based on its content.\n",
    "     - **Regression:** Predicting house prices based on features like square footage, number of bedrooms, etc.\n",
    "\n",
    "   - **Unsupervised Learning:** In unsupervised learning, the algorithm works with unlabeled data and aims to find hidden patterns or structure in the data without explicit guidance. There are no target labels, and the algorithm explores the data's inherent organization. Examples include:\n",
    "     - **Clustering:** Grouping customers into segments based on their purchasing behavior.\n",
    "     - **Dimensionality Reduction:** Reducing the number of features while retaining essential information in a dataset.\n",
    "\n",
    "2. **Unsupervised Learning Applications:**\n",
    "\n",
    "   - **Clustering:** Grouping similar data points together in customer segmentation, image segmentation, or anomaly detection.\n",
    "   - **Dimensionality Reduction:** Reducing high-dimensional data to improve visualization or computational efficiency.\n",
    "   - **Generative Models:** Creating new data instances similar to existing data, such as generating realistic images or text.\n",
    "   - **Association Rule Mining:** Discovering interesting relationships or patterns in transaction data, like market basket analysis.\n",
    "\n",
    "3. **Three Main Types of Clustering Methods:**\n",
    "\n",
    "   - **Hierarchical Clustering:** Creates a hierarchy of clusters, forming a tree-like structure called a dendrogram. It can be agglomerative (bottom-up) or divisive (top-down).\n",
    "   - **Partitioning Clustering:** Divides data into non-overlapping clusters. K-means is a popular partitioning method.\n",
    "   - **Density-Based Clustering:** Identifies dense regions of data points separated by sparser regions. DBSCAN is a well-known density-based algorithm.\n",
    "\n",
    "4. **K-Means Consistency:**\n",
    "   The k-means algorithm determines the consistency of clustering by minimizing the sum of squared distances (SSE) between data points and their assigned cluster centroids. It iteratively updates centroids and assigns data points to the nearest centroid until convergence. A consistent clustering has centroids that are close to the true underlying cluster centers and minimal intra-cluster variance.\n",
    "\n",
    "5. **Difference Between K-Means and K-Medoids:**\n",
    "   - **K-Means:** Uses the mean (centroid) of data points in a cluster as its representative point. It is sensitive to outliers.\n",
    "   - **K-Medoids:** Uses the actual data point within a cluster that minimizes the sum of distances to other points in the cluster as its representative. It is more robust to outliers.\n",
    "\n",
    "6. **Dendrogram:**\n",
    "   A dendrogram is a tree-like diagram used in hierarchical clustering to visualize the clustering process. It displays how data points are grouped into clusters and subclusters at different levels of granularity. Dendrograms are useful for understanding hierarchical relationships in the data.\n",
    "\n",
    "7. **SSE (Sum of Squared Errors):**\n",
    "   SSE is a measure of the goodness of fit in clustering algorithms like k-means. It calculates the sum of squared distances between each data point and its assigned cluster centroid. Lower SSE indicates tighter, more consistent clusters. In k-means, SSE is minimized to find optimal cluster centroids.\n",
    "\n",
    "8. **K-Means Procedure (Step-by-Step):**\n",
    "   1. Initialize K centroids randomly.\n",
    "   2. Assign each data point to the nearest centroid.\n",
    "   3. Recalculate the centroids as the mean of data points in each cluster.\n",
    "   4. Repeat steps 2 and 3 until convergence (when centroids no longer change significantly).\n",
    "   5. The final clusters are formed.\n",
    "\n",
    "9. **Hierarchical Clustering Terms:**\n",
    "   - **Single Linkage:** Also known as \"nearest neighbor linkage,\" it merges clusters based on the minimum distance between any two points from different clusters.\n",
    "   - **Complete Linkage:** Merges clusters based on the maximum distance between any two points from different clusters. It tends to produce compact, spherical clusters.\n",
    "\n",
    "10. **Apriori Concept in Business Basket Analysis:**\n",
    "    Apriori is an algorithm used in market basket analysis to discover associations between items in transaction data. It reduces measurement overhead by setting a minimum support threshold. Only itemsets that meet the minimum support threshold are considered frequent, and association rules are generated only for these itemsets.\n",
    "\n",
    "    Example: In a supermarket, if the minimum support threshold is set at 5%, the Apriori algorithm would identify itemsets (combinations of products) that are purchased together in at least 5% of transactions. This reduces the number of rules generated and focuses on the most relevant associations. For instance, if \"bread\" and \"milk\" are bought together in 10% of transactions, an association rule might suggest: \"If a customer buys bread, they are likely to buy milk.\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337a334b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
