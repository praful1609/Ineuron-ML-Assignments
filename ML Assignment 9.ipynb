{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a99ac81a",
   "metadata": {},
   "source": [
    "Ans1. **Feature Engineering**:\n",
    "\n",
    "   Feature engineering involves creating new features or modifying existing ones to improve the performance of machine learning models. It aims to represent data in a way that is more informative for the specific task. Key aspects include:\n",
    "\n",
    "   - **Imputation**: Handling missing data by filling in or estimating missing values.\n",
    "   - **Scaling and Normalization**: Ensuring features are on similar scales, often important for distance-based algorithms.\n",
    "   - **Binning or Discretization**: Grouping continuous values into intervals or categories.\n",
    "   - **One-Hot Encoding**: Converting categorical variables into binary vectors.\n",
    "   - **Feature Interactions**: Creating new features that represent interactions between existing features.\n",
    "   - **Polynomial Features**: Introducing higher-order terms of existing features.\n",
    "   - **Time-based Features**: Extracting time-related information like day of the week, month, etc.\n",
    "   - **Text Processing**: Techniques like TF-IDF, word embeddings, and N-grams for text data.\n",
    "   \n",
    "Ans2. **Feature Selection**:\n",
    "\n",
    "   Feature selection is the process of choosing a subset of the most relevant features from the original set. The aim is to reduce the dimensionality of the feature space while retaining as much relevant information as possible. Methods include:\n",
    "\n",
    "   - **Filter Methods**: Evaluate features based on statistical properties or relationships with the target variable. (e.g., correlation, mutual information)\n",
    "   - **Wrapper Methods**: Use a predictive model to evaluate different subsets of features. (e.g., forward selection, backward elimination)\n",
    "   - **Embedded Methods**: Features are selected as part of the model training process. (e.g., LASSO regression, tree-based methods)\n",
    "\n",
    "Ans3. **Filter vs. Wrapper Approaches**:\n",
    "\n",
    "   - **Filter Approach**:\n",
    "     - *Pros*: Fast, less computationally intensive, can be applied before model training.\n",
    "     - *Cons*: Ignores feature interactions, may not consider model-specific information.\n",
    "\n",
    "   - **Wrapper Approach**:\n",
    "     - *Pros*: Considers feature interactions, evaluates features in the context of the model.\n",
    "     - *Cons*: Computationally expensive, may lead to overfitting.\n",
    "\n",
    "Ans4. **Overall Feature Selection Process**:\n",
    "\n",
    "   - **Step 1**: Generate a large pool of potential features.\n",
    "   - **Step 2**: Apply a feature selection method (e.g., filter, wrapper, embedded) to evaluate and rank features.\n",
    "   - **Step 3**: Select the top-ranked features based on the evaluation criterion.\n",
    "   - **Step 4**: Optionally, iteratively refine the feature set based on model performance.\n",
    "\n",
    "   **Feature Extraction Example**: In text analysis, feature extraction may involve converting a document into a vector representation using techniques like TF-IDF or word embeddings. These vectors can then be used as features for machine learning models.\n",
    "\n",
    "Ans5. **Text Categorization Feature Engineering**:\n",
    "\n",
    "   - **Tokenization**: Breaking text into words or tokens.\n",
    "   - **Stopword Removal**: Removing common words (e.g., \"the\", \"is\") that do not carry much information.\n",
    "   - **TF-IDF Vectorization**: Assigning weights to words based on their importance in a document.\n",
    "   - **N-grams**: Considering sequences of words (e.g., bigrams, trigrams) as features.\n",
    "\n",
    "Ans6. **Cosine Similarity for Text Categorization**:\n",
    "\n",
    "   Cosine similarity is used because it measures the cosine of the angle between two vectors in a high-dimensional space. It is particularly useful for text categorization because it captures the similarity in terms of the angle between the vectors, not their magnitude. For the given document-term matrix rows:\n",
    "\n",
    "   - Vector 1: [2, 3, 2, 0, 2, 3, 3, 0, 1]\n",
    "   - Vector 2: [2, 1, 0, 0, 3, 2, 1, 3, 1]\n",
    "\n",
    "   The cosine similarity is calculated by taking the dot product of the vectors and dividing by the product of their magnitudes.\n",
    "\n",
    "Ans7. **Hamming Distance and Jaccard Index**:\n",
    "\n",
    "   i. **Hamming Distance** Formula: Number of positions at which corresponding bits are different. For 10001011 and 11001111, the Hamming distance is 3 (positions 3, 4, and 6).\n",
    "\n",
    "   ii. **Jaccard Index**: Measures the similarity between sets. For sets A and B, Jaccard Index = (Intersection of A and B) / (Union of A and B). Given sets (1, 1, 0, 0, 1, 0, 1, 1) and (1, 0, 0, 1, 1, 0, 0, 1), the Jaccard Index is 3/5.\n",
    "\n",
    "Ans8. **High-Dimensional Data Set**:\n",
    "\n",
    "   A high-dimensional dataset has a large number of features relative to the number of samples. Real-life examples include:\n",
    "   - Genomic data with thousands of genes.\n",
    "   - Image data with many pixels.\n",
    "   - Natural language processing with a large vocabulary.\n",
    "\n",
    "   Difficulties in using machine learning techniques on high-dimensional data include increased computational complexity, risk of overfitting, and challenges in visualization. Techniques like dimensionality reduction (e.g., PCA) can help mitigate these challenges.\n",
    "\n",
    "Ans9. **Quick Notes**:\n",
    "\n",
    "   - PCA (Principal Component Analysis): A dimensionality reduction technique that linearly transforms data to a lower-dimensional space while preserving variance.\n",
    "   - Use of Vectors: Vectors are often used to represent data points or features in a mathematical space.\n",
    "   - Embedded Technique: Feature selection methods that are integrated into the model training process.\n",
    "\n",
    "Ans10. **Comparisons**:\n",
    "\n",
    "    - Sequential Backward Exclusion vs. Sequential Forward Selection:\n",
    "      - Backward Exclusion starts with all features and removes one at a time, while Forward Selection starts with no features and adds one at a time.\n",
    "      - Backward Exclusion is computationally less expensive but may not always find the best subset. Forward Selection can be more exhaustive but might be computationally intensive.\n",
    "\n",
    "    - Filter vs. Wrapper Methods for Feature Selection:\n",
    "      - Filter methods evaluate features independently of any specific model, while wrapper methods use a specific model to evaluate feature subsets.\n",
    "      - Filter methods are faster but may miss interactions. Wrapper methods consider feature interactions but are computationally expensive.\n",
    "\n",
    "    - SMC vs. Jaccard Coefficient:\n",
    "      - SMC (Simple Matching Coefficient) measures the similarity between binary vectors based on the proportion of matching elements.\n",
    "      - Jaccard Coefficient measures the similarity between sets based on the ratio of the size of the intersection to the size of the union."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5740e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
