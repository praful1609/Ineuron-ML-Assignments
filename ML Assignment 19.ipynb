{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ef79b0d",
   "metadata": {},
   "source": [
    "1. **K-Means Clustering:**\n",
    "\n",
    "   a) Using the given data points and initial centroids:\n",
    "   \n",
    "      Set 1: Centroids (15, 32) and (12, 30)\n",
    "      - Iteration 1:\n",
    "        Cluster 1: {5, 10, 15, 20, 25}\n",
    "        Cluster 2: {30, 35}\n",
    "      - Iteration 2 (centroids update):\n",
    "        New centroid 1: (15, 16)\n",
    "        New centroid 2: (32, 32)\n",
    "        Cluster 1: {5, 10, 15, 20, 25}\n",
    "        Cluster 2: {30, 35}\n",
    "      - Iteration 3 (no change in centroids, convergence):\n",
    "        Cluster 1: {5, 10, 15, 20, 25}\n",
    "        Cluster 2: {30, 35}\n",
    "   \n",
    "      Set 2: Centroids (12, 30) and (15, 32)\n",
    "      - Iteration 1:\n",
    "        Cluster 1: {5, 10}\n",
    "        Cluster 2: {15, 20, 25, 30, 35}\n",
    "      - Iteration 2 (centroids update):\n",
    "        New centroid 1: (7.5, 7.5)\n",
    "        New centroid 2: (27, 32)\n",
    "        Cluster 1: {5, 10}\n",
    "        Cluster 2: {15, 20, 25, 30, 35}\n",
    "      - Iteration 3 (no change in centroids, convergence):\n",
    "        Cluster 1: {5, 10}\n",
    "        Cluster 2: {15, 20, 25, 30, 35}\n",
    "   \n",
    "   b) SSE Calculation:\n",
    "      SSE measures the sum of squared distances between data points and their assigned cluster centroids.\n",
    "\n",
    "      For Set 1 (Centroids: (15, 16) and (32, 32)):\n",
    "      - SSE for Cluster 1 = (0^2 + 5^2 + 10^2 + 5^2 + 10^2) = 250\n",
    "      - SSE for Cluster 2 = (2^2 + 3^2) = 13\n",
    "      Total SSE = 250 + 13 = 263\n",
    "\n",
    "      For Set 2 (Centroids: (7.5, 7.5) and (27, 32)):\n",
    "      - SSE for Cluster 1 = (2.5^2 + 2.5^2) = 12.5\n",
    "      - SSE for Cluster 2 = (7.5^2 + 7.5^2 + 2.5^2 + 2.5^2 + 2.5^2) = 93.75\n",
    "      Total SSE = 12.5 + 93.75 = 106.25\n",
    "\n",
    "2. **Market Basket Research and Association Analysis:**\n",
    "   - Market Basket Research uses association analysis to discover relationships between items purchased together in transactions.\n",
    "   - Association analysis identifies frequent itemsets and generates association rules (e.g., \"If item A is bought, then item B is also likely to be bought\").\n",
    "   - It helps retailers understand customer buying patterns, optimize product placement, and plan promotions effectively.\n",
    "\n",
    "3. **Apriori Algorithm Example:**\n",
    "   Suppose you have a transaction dataset with items purchased:\n",
    "   Transaction 1: {Milk, Bread, Eggs}\n",
    "   Transaction 2: {Milk, Bread, Diapers}\n",
    "   Transaction 3: {Bread, Diapers, Beer}\n",
    "   Transaction 4: {Milk, Bread}\n",
    "   \n",
    "   Using the Apriori algorithm:\n",
    "   - Set minimum support (e.g., 50%): Itemsets {Milk}, {Bread}, {Diapers}, {Bread, Milk} are frequent.\n",
    "   - Generate association rules: {Milk} => {Bread}, {Bread} => {Milk}, {Diapers} => {Bread}, {Bread} => {Diapers}, {Milk, Bread} => {Diapers}.\n",
    "\n",
    "4. **Distance Measurement in Hierarchical Clustering:**\n",
    "   - In hierarchical clustering, the distance between clusters is measured using various metrics, such as:\n",
    "     - Single Linkage (nearest neighbor): Minimum distance between any two points from different clusters.\n",
    "     - Complete Linkage (furthest neighbor): Maximum distance between any two points from different clusters.\n",
    "     - Average Linkage: Average distance between all pairs of points from different clusters.\n",
    "   - The choice of distance metric affects the clustering results.\n",
    "   - Iterations continue until a stopping criterion is met, such as a specified number of clusters or a distance threshold.\n",
    "\n",
    "5. **Recomputing Cluster Centroids in K-Means:**\n",
    "   - In K-means, cluster centroids are recomputed in each iteration.\n",
    "   - The new centroids are calculated as the mean (average) of data points within each cluster.\n",
    "   - The centroid represents the center of the cluster and is used to assign data points to the nearest cluster.\n",
    "\n",
    "6. **Determining the Number of Clusters:**\n",
    "   - Elbow Method: Plot SSE against the number of clusters and look for an \"elbow\" point where SSE starts to level off. It indicates a reasonable number of clusters.\n",
    "   - Silhouette Method: Measure how similar each data point is to its own cluster compared to other clusters. Maximize the silhouette score.\n",
    "   - Gap Statistics: Compare SSE of the clustering to a reference distribution to find an optimal number of clusters.\n",
    "\n",
    "7. **K-Means Advantages and Disadvantages:**\n",
    "   - Advantages:\n",
    "     - Simple and easy to implement.\n",
    "     - Scalable for large datasets.\n",
    "     - Works well when clusters are spherical and have similar sizes.\n",
    "   - Disadvantages:\n",
    "     - Sensitive to initial centroid positions.\n",
    "     - Assumes clusters are of similar density and size.\n",
    "     - Struggles with non-linear or irregularly shaped clusters.\n",
    "     - May converge to local optima.\n",
    "\n",
    "8. **Diagram for Clustering:**\n",
    "   (Unfortunately, I cannot draw diagrams as a text-based AI, but you can create visual diagrams using various software tools or draw them by hand to illustrate clustering principles.)\n",
    "\n",
    "9. **K-Means Clustering (Second Iteration):**\n",
    "   If you were to run a second iteration, you would update the cluster centroids based on the data points assigned to each cluster. After updating centroids, you would reassign data points to the nearest centroid. The new clustering result would likely lead to different centroids and potentially different SSE values.\n",
    "\n",
    "10. **Software Defect Clustering:**\n",
    "    In this scenario, you have 20 defect data points clustered into 5 clusters using the k-means algorithm. To illustrate the process, you can create a diagram showing the 5 clusters with labeled data points within each cluster. Additionally, you can use different shapes or colors to represent the clusters and their boundaries to visually demonstrate the clustering results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a528dff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
